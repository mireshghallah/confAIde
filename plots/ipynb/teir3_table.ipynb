{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class smart_dict(dict):\n",
    "    @staticmethod\n",
    "    def __missing__(key):\n",
    "        return key\n",
    "\n",
    "def create_save_csv(metrics=['free-response_metrics_string-match','free-response_metrics_proxy-model'], models=['gpt-4-0613','gpt-3.5-turbo-0613','text-davinci-003','text-curie-001','llama-2-70b-chat-tg','llama-2-70b-tg','flan-ul2'],pr='with'):\n",
    "    model_dict = {'gpt-4-0613':'GPT4','gpt-3.5-turbo-0613':'ChatGPT','text-davinci-003':'Davinci','text-curie-001':'Curie','llama-2-70b-chat-tg':'Llama2 Chat','llama-2-70b-tg':'Llama2','flan-ul2':'Flan UL2' }\n",
    "    topic_dict = smart_dict({'work/academic cheating':'Cheating','belief/ideology':'religion'})\n",
    "    incentive_dict = smart_dict({'to gain financial advantage/money through bonus':'bonus', 'to gain financial advantage/money through wager':'wager'})\n",
    "    metric_dict = smart_dict({'free-response_metrics_string-match':'Leakage thru. String Match','free-response_metrics_proxy-model':'Leakage thru. Proxy Agent', 'binary_metrics_error':'Binary Control Question', 'info-accessibility_metrics_error':'Information Access. Err.' , 'privacy-sharing_metrics_error': 'Private Information Access. Err.' })\n",
    "    #csv_out = open(,'w')\n",
    "    # writer = csv.writer(csv_out)\n",
    "    # writer.writerow(['scenario_idx']+[model_dict[model] for model in models])\n",
    "    df_all = pd.DataFrame(columns = ['Metric']+['Case']+[model_dict[model] for model in models])\n",
    "    # Using DataFrame.insert() to add a column\n",
    "    for j,metric in enumerate(metrics):\n",
    "        list_avg = [f'{metric_dict[metric]}','avg']\n",
    "        list_worst = [f'{metric_dict[metric]}','worst']\n",
    "\n",
    "        for model in models:\n",
    "            json_file_name = f'../../eval_results/{pr}_privacy_prompts/eval_{model}_data_tier_3_nsamples_10_q_{metric}.json'\n",
    "            with open(json_file_name,'r') as jfile:\n",
    "                data = json.load(jfile)\n",
    "            \n",
    "            \n",
    "            #df_temp.loc[i] = []\n",
    "            worst_case = 0\n",
    "            worst_case_cnt = 0\n",
    "            idx = 0\n",
    "            avg_case = 0\n",
    "            \n",
    "            for i , s_d in enumerate(data):\n",
    "                if s_d['eval_result'] :\n",
    "                        worst_case = 1\n",
    "                        avg_case +=1\n",
    "           \n",
    "                if i%10 == 9:\n",
    "                    #write down:\n",
    "                    worst_case_cnt += worst_case\n",
    "                    idx+=1\n",
    "                    worst_case = 0\n",
    "                \n",
    "            ## push the metrics\n",
    "            list_avg.append(avg_case/len(data))\n",
    "            list_worst.append(worst_case_cnt/(len(data)//10))\n",
    "\n",
    "                    \n",
    "        \n",
    "        \n",
    "        df_all.loc[2*j] = list_avg\n",
    "        df_all.loc[2*j+1] = list_worst\n",
    "\n",
    "    df_all.to_csv(f'../csv/tier3_table_avg_{pr}.csv')\n",
    "    return df_all\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tab_to_latext(df):\n",
    "    for i in range(len(df)):\n",
    "        list_r = df.loc[i].to_list()[1:]\n",
    "        list_r = [list_r[0].replace('_','-')] + [ '%.2f' % elem for elem in list_r[1:]]\n",
    "        min_item = min(list_r[1:])\n",
    "        list_row = [str(item).replace('_','-')  if item != min_item  else f\"\\\\textbf{{{item}}}\"  for item in list_r  ]\n",
    "        print('& ',' & '.join(list_row),' \\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with privacy prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = create_save_csv(['free-response_metrics_string-match','free-response_metrics_proxy-model','info-accessibility_metrics_error','privacy-sharing_metrics_error','binary_metrics_error'],pr='with')\n",
    "df_all_sorted = df_all.sort_values(by='Case').reset_index().drop(columns=['Case','Curie']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&  Leakage thru. String Match & \\textbf{0.33} & 0.71 & 0.55 & 0.75 & 0.61 & 0.62  \\\\\n",
      "&  Leakage thru. Proxy Agent & \\textbf{0.26} & 0.56 & 0.44 & 0.51 & 0.38 & 0.42  \\\\\n",
      "&  Leakage thru. String Match & \\textbf{0.54} & 0.95 & 0.88 & 1.00 & 0.99 & 0.98  \\\\\n",
      "&  Leakage thru. Proxy Agent & \\textbf{0.48} & 0.91 & 0.84 & 0.99 & 0.97 & 0.95  \\\\\n"
     ]
    }
   ],
   "source": [
    "tab_to_latext(df_all_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Case</th>\n",
       "      <th>GPT4</th>\n",
       "      <th>ChatGPT</th>\n",
       "      <th>Davinci</th>\n",
       "      <th>Curie</th>\n",
       "      <th>Llama2 Chat</th>\n",
       "      <th>Llama2</th>\n",
       "      <th>Flan UL2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Leakage thru. String Match</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.331852</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.546296</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.747407</td>\n",
       "      <td>0.608519</td>\n",
       "      <td>0.617407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leakage thru. String Match</td>\n",
       "      <td>worst</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.951852</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leakage thru. Proxy Agent</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.260370</td>\n",
       "      <td>0.555185</td>\n",
       "      <td>0.441852</td>\n",
       "      <td>0.329630</td>\n",
       "      <td>0.508148</td>\n",
       "      <td>0.381852</td>\n",
       "      <td>0.417407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leakage thru. Proxy Agent</td>\n",
       "      <td>worst</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.985185</td>\n",
       "      <td>0.974074</td>\n",
       "      <td>0.948148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Metric   Case      GPT4   ChatGPT   Davinci     Curie  \\\n",
       "0  Leakage thru. String Match    avg  0.331852  0.705556  0.546296  0.480000   \n",
       "1  Leakage thru. String Match  worst  0.540741  0.951852  0.877778  0.907407   \n",
       "2   Leakage thru. Proxy Agent    avg  0.260370  0.555185  0.441852  0.329630   \n",
       "3   Leakage thru. Proxy Agent  worst  0.477778  0.911111  0.844444  0.837037   \n",
       "\n",
       "   Llama2 Chat    Llama2  Flan UL2  \n",
       "0     0.747407  0.608519  0.617407  \n",
       "1     0.996296  0.992593  0.981481  \n",
       "2     0.508148  0.381852  0.417407  \n",
       "3     0.985185  0.974074  0.948148  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WO privacy prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../eval_results/without_privacy_prompts/eval_gpt-4-0613_data_tier_3_nsamples_10_q_info-accessibility_metrics_error.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_all \u001b[39m=\u001b[39m create_save_csv([\u001b[39m'\u001b[39;49m\u001b[39mfree-response_metrics_string-match\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mfree-response_metrics_proxy-model\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39minfo-accessibility_metrics_error\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mprivacy-sharing_metrics_error\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mbinary_metrics_error\u001b[39;49m\u001b[39m'\u001b[39;49m],pr\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwithout\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m df_all_sorted \u001b[39m=\u001b[39m df_all\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCase\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index()\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mCase\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mCurie\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m, in \u001b[0;36mcreate_save_csv\u001b[0;34m(metrics, models, pr)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[1;32m     21\u001b[0m     json_file_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../../eval_results/\u001b[39m\u001b[39m{\u001b[39;00mpr\u001b[39m}\u001b[39;00m\u001b[39m_privacy_prompts/eval_\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m_data_tier_3_nsamples_10_q_\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(json_file_name,\u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m jfile:\n\u001b[1;32m     23\u001b[0m         data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(jfile)\n\u001b[1;32m     26\u001b[0m     \u001b[39m#df_temp.loc[i] = []\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../eval_results/without_privacy_prompts/eval_gpt-4-0613_data_tier_3_nsamples_10_q_info-accessibility_metrics_error.json'"
     ]
    }
   ],
   "source": [
    "df_all = create_save_csv(['free-response_metrics_string-match','free-response_metrics_proxy-model','info-accessibility_metrics_error','privacy-sharing_metrics_error','binary_metrics_error'],pr='without')\n",
    "df_all_sorted = df_all.sort_values(by='Case').reset_index().drop(columns=['Case','Curie']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&  Leakage thru. String Match & \\textbf{0.09} & 0.52 & 0.34 & 0.82 & 0.52 & 0.65  \\\\\n",
      "&  Leakage thru. Proxy Agent & \\textbf{0.07} & 0.40 & 0.26 & 0.53 & 0.30 & 0.46  \\\\\n",
      "&  Information Access. Err. & \\textbf{0.02} & 0.12 & 0.40 & 0.86 & 0.79 & 0.16  \\\\\n",
      "&  Private Information Access. Err. & \\textbf{0.02} & 0.09 & 0.31 & 0.83 & 0.76 & 0.12  \\\\\n",
      "&  Binary Control Question & 0.04 & 0.01 & \\textbf{0.00} & 0.39 & 0.78 & 0.36  \\\\\n",
      "&  Leakage thru. String Match & \\textbf{0.22} & 0.93 & 0.79 & 1.00 & 0.99 & 0.99  \\\\\n",
      "&  Leakage thru. Proxy Agent & \\textbf{0.20} & 0.89 & 0.74 & 0.99 & 0.96 & 0.97  \\\\\n",
      "&  Information Access. Err. & \\textbf{0.04} & 0.40 & 0.76 & 1.00 & 1.00 & 0.60  \\\\\n",
      "&  Private Information Access. Err. & \\textbf{0.03} & 0.32 & 0.70 & 1.00 & 1.00 & 0.56  \\\\\n",
      "&  Binary Control Question & 0.06 & 0.04 & \\textbf{0.00} & 0.99 & 1.00 & 0.91  \\\\\n"
     ]
    }
   ],
   "source": [
    "tab_to_latext(df_all_sorted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
